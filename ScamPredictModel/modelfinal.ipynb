{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdBD1-joeoiV",
        "outputId": "dd5937cf-099f-4835-edfa-3ee03b9c02a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns found in dataset: Index(['text', 'is_spam'], dtype='object')\n",
            "Model Accuracy: 0.9551\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96      5104\n",
            "           1       0.95      0.94      0.94      3420\n",
            "\n",
            "    accuracy                           0.96      8524\n",
            "   macro avg       0.95      0.95      0.95      8524\n",
            "weighted avg       0.96      0.96      0.96      8524\n",
            "\n",
            "Model saved successfully.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5032\n",
            " * Running on http://172.28.0.12:5032\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Paths for model, vectorizer, and dataset\n",
        "BASE_DIR = os.getcwd()\n",
        "MODEL_PATH = os.path.join(BASE_DIR, \"scam_detection_model.pkl\")\n",
        "VECTORIZER_PATH = os.path.join(BASE_DIR, \"vectorizer.pkl\")\n",
        "DATASET_PATH = os.path.join(BASE_DIR, \"junkmail_dataset.csv\")  # Uses uploaded dataset\n",
        "\n",
        "# Load dataset and check columns\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(\"Columns found in dataset:\", df.columns)\n",
        "\n",
        "# Ensure dataset contains the expected columns\n",
        "if \"text\" not in df.columns or \"is_spam\" not in df.columns:\n",
        "    raise ValueError(\" Required columns 'text' or 'is_spam' not found in dataset. Check CSV format.\")\n",
        "\n",
        "# Convert labels: 0 = safe, 1 = scam\n",
        "df[\"label\"] = df[\"is_spam\"].astype(int)\n",
        "\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Save trained model only if accuracy is acceptable\n",
        "if accuracy > 0.75:  # Adjust threshold as needed\n",
        "    joblib.dump(model, MODEL_PATH)\n",
        "    joblib.dump(vectorizer, VECTORIZER_PATH)\n",
        "    print(\"Model saved successfully.\")\n",
        "else:\n",
        "    print(\" Model accuracy is too low. Consider improving dataset or model choice.\")\n",
        "\n",
        "\n",
        "# Save trained model\n",
        "joblib.dump(model, MODEL_PATH)\n",
        "joblib.dump(vectorizer, VECTORIZER_PATH)\n",
        "\n",
        "# Scam keywords for extra scoring\n",
        "SCAM_KEYWORDS = {\n",
        "    \"link in bio\", \"bio\", \"giveaway\", \"airdrop\", \"win\", \"prize\", \"free\", \"click here\",\n",
        "    \"claim now\", \"bitcoin\", \"binance\", \"eth\", \"doge\", \"roobet\", \"lottery\", \"bonus\",\n",
        "    \"btc\", \"investment\", \"fast money\", \"double your money\", \"instant profit\", \"stake\"\n",
        "}\n",
        "\n",
        "# Track repeat scam content\n",
        "scam_counts = Counter()\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans text by removing links, special characters, and stopwords.\"\"\"\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    words = text.lower().split()\n",
        "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "@app.route(\"/analyze\", methods=[\"POST\"])\n",
        "def analyze():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        if not data or \"postDescription\" not in data or \"imageText\" not in data:\n",
        "            return jsonify({\"error\": \"Provide 'postDescription' and 'imageText'\"}), 400\n",
        "\n",
        "        text = f\"{data['imageText']} {data['postDescription']}\"\n",
        "        cleaned_text = preprocess_text(text)\n",
        "        transformed_text = vectorizer.transform([cleaned_text])\n",
        "\n",
        "        # Predict scam probability\n",
        "        prob = model.predict_proba(transformed_text)[0]\n",
        "        risk_score = prob[1] if len(prob) > 1 else prob[0]\n",
        "\n",
        "        # Scam keyword detection\n",
        "        scam_keywords_found = [word for word in SCAM_KEYWORDS if word in cleaned_text]\n",
        "        scam_likelihood = \"High\" if risk_score > 0.7 else \"Medium\" if risk_score > 0.4 else \"Low\"\n",
        "\n",
        "        # Track repeat scam patterns\n",
        "        if scam_likelihood == \"High\":\n",
        "            scam_counts[text] += 1\n",
        "\n",
        "        return jsonify({\n",
        "            \"risk_score\": round(risk_score * 100, 2),\n",
        "            \"scam_keywords\": scam_keywords_found,\n",
        "            \"scam_likelihood\": scam_likelihood,\n",
        "            \"repeat_scam_count\": scam_counts[text]\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Unexpected error: {str(e)}\"}), 500\n",
        "\n",
        "\n",
        "@app.route(\"/flag\", methods=[\"POST\"])\n",
        "def flag():\n",
        "    \"\"\"Adds flagged content to the dataset and retrains the model.\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        if not data or \"postDescription\" not in data or \"imageText\" not in data:\n",
        "            return jsonify({\"error\": \"Provide 'postDescription' and 'imageText'\"}), 400\n",
        "\n",
        "        text = f\"{data['imageText']} {data['postDescription']}\"\n",
        "        cleaned_text = preprocess_text(text)\n",
        "\n",
        "        # Append to dataset\n",
        "        global df\n",
        "        new_entry = pd.DataFrame([[cleaned_text, 1]], columns=[\"text\", \"label\"])\n",
        "        df = pd.concat([df, new_entry], ignore_index=True)\n",
        "\n",
        "        # Ensure dataset has both scam (1) and non-scam (0) data\n",
        "        if df[\"label\"].nunique() < 2:\n",
        "            print(\"Not enough class diversity. Adding sample non-scam data.\")\n",
        "            df = pd.concat([df, pd.DataFrame([\n",
        "                [\"Genuine tech discussion on AI\", 0],\n",
        "                [\"Football match highlights\", 0],\n",
        "                [\"Latest iPhone features\", 0],\n",
        "                [\"Stock market analysis\", 0]\n",
        "            ], columns=[\"text\", \"label\"])], ignore_index=True)\n",
        "\n",
        "        # Save updated dataset\n",
        "        df.to_csv(DATASET_PATH, index=False)\n",
        "\n",
        "        # Retrain model\n",
        "        retrain_model()\n",
        "\n",
        "        return jsonify({\"message\": \"Post flagged and added to dataset. Model updated.\"})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Unexpected error: {str(e)}\"})\n",
        "\n",
        "\n",
        "def retrain_model():\n",
        "    \"\"\"Retrains the model with the updated dataset.\"\"\"\n",
        "    try:\n",
        "        global df\n",
        "        if df.empty:\n",
        "            print(\"Dataset is empty, skipping retraining.\")\n",
        "            return\n",
        "\n",
        "        texts = df[\"text\"].values\n",
        "        labels = df[\"label\"].values\n",
        "\n",
        "        # Refit vectorizer with the entire dataset\n",
        "        global vectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X = vectorizer.fit_transform(texts)\n",
        "\n",
        "        # Train new model\n",
        "        global model\n",
        "        model = LogisticRegression()\n",
        "        model.fit(X, labels)\n",
        "\n",
        "        # Save updated model and vectorizer\n",
        "        joblib.dump(model, MODEL_PATH)\n",
        "        joblib.dump(vectorizer, VECTORIZER_PATH)\n",
        "\n",
        "        print(\"Model retrained successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Retraining failed: {str(e)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(\n",
        "        host=\"0.0.0.0\",\n",
        "        port=5032\n",
        "    )\n"
      ]
    }
  ]
}